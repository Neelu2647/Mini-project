# IMAGE CLASSIFICATION & EXPLAINABLE VISION 

##  Objective
This project focuses on developing deep learning solutions for multi-class image classification tasks, with an emphasis on improving both performance and interpretability. To enhance model transparency, techniques such as Grad-CAM, saliency maps, and attention mechanisms are utilized to visualize and understand predictions. Additionally, to address potential class imbalance, data augmentation methods (e.g., rotation, flipping, and color jittering) are employed alongside resampling strategies like oversampling minority classes or undersampling majority classes. By integrating these approaches, the goal is to create robust and interpretable models that provide reliable predictions while offering deeper insights into their decision-making processes.

## Team Members
22052657
22052647
22052641 
22053171
2205746

## Features
1. Image Preprocessing: Resize, crop, normalize pixel values, and handle color channels.
2. Model Development: Build CNNs from scratch or use pre-trained models like ResNet or VGG. Consider using ensemble methods to improve accuracy.
3. Class Imbalance Solutions: Apply data augmentation techniques to increase the number of samples from the minority class.
4. Explainability: Use tools like Grad-CAM or saliency maps to visualize which parts of the image the model focuses on during decision-making.
5. Performance Metrics: Evaluate the model with accuracy, F1-score, confusion matrices, and plot loss/accuracy over epochs.
