# IMAGE CLASSIFICATION & EXPLAINABLE VISION 

##  Objective
This project develops deep learning solutions for multi-class image classification, focusing on interpretability and class imbalance. Techniques like Grad-CAM and saliency maps enhance transparency, while data augmentation and resampling improve class distribution. The goal is to build robust, interpretable models with reliable predictions.

## Team Members
22052657
22052647
22052641 
22053171
2205746

## Features
1. Image Preprocessing: Resize, crop, normalize pixel values, and handle color channels.
2. Model Development: Build CNNs from scratch or use pre-trained models like ResNet or VGG. Consider using ensemble methods to improve accuracy.
3. Class Imbalance Solutions: Apply data augmentation techniques to increase the number of samples from the minority class.
4. Explainability: Use tools like Grad-CAM or saliency maps to visualize which parts of the image the model focuses on during decision-making.
5. Performance Metrics: Evaluate the model with accuracy, F1-score, confusion matrices, and plot loss/accuracy over epochs.
